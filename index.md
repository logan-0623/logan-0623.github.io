---
layout: default
title: Home Page
---

## **Zihong Luo**

<div style="margin-bottom: 1.5em; line-height: 1.8;">
  <strong>Undergraduate Researcher</strong> @ UoL / XJTLU<br>
  <strong>Research Interests:</strong> Embodied AI Â· Multimodal Perception Â· Robotic Manipulation<br>
  Currently working on VLA policies and Sim-to-Real transfer at <strong>SmartLab</strong>
</div>

**ğŸ“§ Email:** [Z.Luo21@student.liverpool.ac.uk](mailto:Z.Luo21@student.liverpool.ac.uk)

**ğŸ”— Links:** 
[Google Scholar](https://scholar.google.com/citations?hl=zh-CN&user=8SLtiQgAAAAJ&view_op=list_works&authuser=1&gmla=AKzYXQ1L5pBbS_4f0os64QpqQeiBfopt2hbD7zfZHei_uuz3OS4Oz7sMa1vz4xnMsEXGX4wMRhp49fNS00ZliVKgiQjXQOmCNXlBREs9OnpRLYZvVFM) Â· [GitHub](https://github.com/logan-0623) Â·[CV](/cv/) 

---

## ğŸ“° News
- **Sep 2025**: Joined SmartLab working on **gripper-aware VLA policies** for robotic manipulation
- **Aug 2025**: Completed research internship at **Jifu Medical** (Robotics Algorithm Team)
- **Jun 2025**: One paper on **Prior-Guided SAM** accepted to **BIBM 2025**
- **Jan 2025**: Advanced to **WorldQuant IQC 2025 UK Finals** (Top 6, Top 0.1% of 79,000+ participants)
- **Dec 2024**: One paper on **Disentangled Representation** accepted to **AAAI 2025**! ğŸ‰
---

## ğŸ¯ Seeking Opportunities

I am actively seeking **Fall 2026 Master programs** in **Robotics/ME/ECE**, with interests in:
- Vision-Language-Action Models & Embodied Intelligence
- Multi-modal Learning for Robotic Manipulation
- Hardware-Aware AI Systems & Real-World Robot Deployment

**Contact:** [Z.Luo21@student.liverpool.ac.uk](mailto:Z.Luo21@student.liverpool.ac.uk) | [Download CV â†’](/cv/)

---

## ğŸ”¬ Featured Research

### **GVLA: Gripper-Aware Vision-Language-Action Policy**
*SmartLab, University of Liverpool (Oct 2024 - Present)*

A gripper-aware VLA policy enabling **zero-shot transfer** across end-effectors. Built hardware-aware multimodal systems using Mixture-of-Experts architecture for robotic manipulation.

**ğŸ¥ Demo coming soon** | [Project Details â†’](/research/#gvla-gripper-aware-vision-language-action-policy-learning)

---

### **Incomplete Modality Disentangled Representation**
*AAAI 2025 (CCF-A)*

Developed **robust representation learning** algorithms to handle incomplete modalities and noisy data. Proposed feature disentanglement methods for modal-common and modal-specific components (Applied to Ophthalmic diagnosis).

ğŸ“„ [Paper](https://openreview.net/forum?id=IlJw8PAYYS) | [View Research â†’](/research/#multi-modal-deletion-completion-of-oct-and-fundus-medical-images)

---

### **PG-SAM: Prior-Guided Medical Segmentation**
*BIBM 2025*

Prior-guided SAM framework leveraging medical LLMs to align high-level textual priors with pixel-level image cues for **multi-organ segmentation**. Demonstrated robust multi-modal fusion capabilities.

ğŸ“„ [ArXiv](https://arxiv.org/abs/2503.18227) | ğŸ’» [Code](https://github.com/your-repo) | [View Research â†’](/research/#pg-sam-prior-guided-sam-with-medical-for-multi-organ-segmentation)

---

## ğŸ› ï¸ Engineering Projects

### **LeRobot Implementation** | [Details â†’](/lerobot-so101/)
Reproduced **SOTA algorithms (ACT/ALOHA)** on real hardware from scratch. Built dual-arm teleoperation system with synchronized dual-camera setup and collected 50+ demonstration episodes.

**ğŸ¥ Demo:** *Robot manipulation GIF/Video placeholder*

---

### **Wheel-legged Robot** | [Details â†’](/robotics/wheel-legged-robot/)
Built a **self-balancing robot** from scratch using ESP32 and PID control. Implemented complete inverse kinematics solver and cascaded PID control for wheel balancing. *Demonstrates strong hardware/software integration skills.*

**ğŸ¥ Demo:** *Self-balancing robot GIF placeholder*

---

## ğŸ“„ Quick Links

- [About Me â†’](/about/) - Academic journey and background
- [Publications â†’](/publications/) - Full publication list (6 papers)
- [All Projects â†’](/projects/) - Complete project portfolio
- [Blog â†’](/blog/) - Research insights and technical posts
- [Download CV â†’](/cv/) - Complete academic profile

