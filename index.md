---
layout: default
title: Home Page
---

## **Zihong Luo**

<div style="margin-bottom: 1.5em; line-height: 1.8;">
  <strong>Undergraduate Researcher</strong> @ UoL / XJTLU<br>
  <strong>Research Interests:</strong> Embodied AI 路 Multimodal Perception 路 Robotic Manipulation<br>
  Currently working on VLA policies and Sim-to-Real transfer at <strong><a href="https://smartlab.csc.liv.ac.uk/">SmartLab</a></strong>
</div>

**Email:** [Z.Luo21@student.liverpool.ac.uk](mailto:Z.Luo21@student.liverpool.ac.uk)

**Links:** 
[Google Scholar](https://scholar.google.com/citations?hl=zh-CN&user=8SLtiQgAAAAJ&view_op=list_works&authuser=1&gmla=AKzYXQ1L5pBbS_4f0os64QpqQeiBfopt2hbD7zfZHei_uuz3OS4Oz7sMa1vz4xnMsEXGX4wMRhp49fNS00ZliVKgiQjXQOmCNXlBREs9OnpRLYZvVFM) 路 [GitHub](https://github.com/logan-0623) 路[CV](/cv/) 

---

## News
- **Sep 2025**: Joined SmartLab working on **gripper-aware VLA policies** for robotic manipulation
- **Aug 2025**: Completed research internship at **Jifu Medical** (Robotics Algorithm Team)
- **Jun 2025**: One paper on **Prior-Guided SAM** accepted to **BIBM 2025**
- **Jan 2025**: Advanced to **WorldQuant IQC 2025 UK Finals** (Top 6, Top 0.1% of 79,000+ participants)
- **Dec 2024**: One paper on **Disentangled Representation** accepted to **AAAI 2025**! 

---

## Seeking Opportunities

I am actively seeking **Fall 2026 Master programs** in **Robotics/ME/ECE**, with interests in:
- Vision-Language-Action Models & Embodied Intelligence
- Multi-modal Learning for Robotic Manipulation
- Hardware-Aware AI Systems & Real-World Robot Deployment

**Contact:** [Z.Luo21@student.liverpool.ac.uk](mailto:Z.Luo21@student.liverpool.ac.uk) | [Download CV ](/cv/)

---

## Featured Research

### **GVLA: Gripper-Aware Vision-Language-Action Policy**
*SmartLab, University of Liverpool (Oct 2024 - Present)*

A gripper-aware VLA policy enabling **zero-shot transfer** across end-effectors. Built hardware-aware multimodal systems using Mixture-of-Experts architecture for robotic manipulation.

**Demo coming soon** | [Project Details ](/research/#gvla-gripper-aware-vision-language-action-policy-learning)

---

### **Incomplete Modality Disentangled Representation**
*AAAI 2025 (CCF-A)*

Developed **robust representation learning** algorithms to handle incomplete modalities and noisy data. Proposed feature disentanglement methods for modal-common and modal-specific components (Applied to Ophthalmic diagnosis).

 [Paper](https://openreview.net/forum?id=IlJw8PAYYS) | [View Research ](/research/#multi-modal-deletion-completion-of-oct-and-fundus-medical-images)

---

### **PG-SAM: Prior-Guided Medical Segmentation**
*BIBM 2025*

Prior-guided SAM framework leveraging medical LLMs to align high-level textual priors with pixel-level image cues for **multi-organ segmentation**. Demonstrated robust multi-modal fusion capabilities.

 [ArXiv](https://arxiv.org/abs/2503.18227) |  [Code](https://github.com/your-repo) | [View Research ](/research/#pg-sam-prior-guided-sam-with-medical-for-multi-organ-segmentation)

---

## Engineering Projects

### **LeRobot Implementation** | [Details ](/lerobot-so101/)
Reproduced **SOTA algorithms (ACT/ALOHA)** on real hardware from scratch. Built dual-arm teleoperation system with synchronized dual-camera setup and collected 50+ demonstration episodes.

** Demo:** *Robot manipulation GIF/Video placeholder*

---

### **Wheel-legged Robot** | [Details ](/robotics/wheel-legged-robot/)
Built a **self-balancing robot** from scratch using ESP32 and PID control. Implemented complete inverse kinematics solver and cascaded PID control for wheel balancing. *Demonstrates strong hardware/software integration skills.*

** Demo:** *Self-balancing robot GIF placeholder*

---

##  Quick Links

- [About Me ](/about/) - Academic journey and background
- [Publications ](/publications/) - Full publication list
- [All Projects ](/projects/) - Complete project portfolio
- [Blog ](/blog/) - Research insights and technical posts
- [Download CV ](/cv/) - Complete academic profile

